import { useState, useRef, useEffect } from 'react'
import { tokenize, tokensToFurigana, speak } from '../utils/japanese'

export default function CameraMode({ deeplKey, onBack }) {
  const [isStreaming, setIsStreaming] = useState(false);
  const [isProcessing, setIsProcessing] = useState(false);
  const [furiganaData, setFuriganaData] = useState([]);
  const [recognizedText, setRecognizedText] = useState('');
  const [showResults, setShowResults] = useState(false);
  const videoRef = useRef(null);
  const canvasRef = useRef(null);
  const streamRef = useRef(null);

  const startCamera = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({
        video: { facingMode: 'environment', width: { ideal: 1280 }, height: { ideal: 720 } },
      });
      if (videoRef.current) {
        videoRef.current.srcObject = stream;
      }
      streamRef.current = stream;
      setIsStreaming(true);
      setShowResults(false);
      setFuriganaData([]);
      setRecognizedText('');
    } catch (err) {
      console.error('Camera error:', err);
      alert('ã‚«ãƒ¡ãƒ©ã‚’ã€€ã²ã‚‰ã‘ã¾ã›ã‚“ã§ã—ãŸ');
    }
  };

  const stopCamera = () => {
    if (streamRef.current) {
      streamRef.current.getTracks().forEach((track) => track.stop());
      streamRef.current = null;
    }
    setIsStreaming(false);
  };

  const stopAndAnalyze = async () => {
    if (!videoRef.current || !canvasRef.current) return;

    setIsProcessing(true);

    const video = videoRef.current;
    const canvas = canvasRef.current;
    const ctx = canvas.getContext('2d');

    // Capture the center box area (60% width, 40% height)
    const boxWidth = video.videoWidth * 0.6;
    const boxHeight = video.videoHeight * 0.4;
    const boxX = (video.videoWidth - boxWidth) / 2;
    const boxY = (video.videoHeight - boxHeight) / 2;

    canvas.width = boxWidth;
    canvas.height = boxHeight;

    // Apply contrast enhancement for better OCR
    ctx.filter = 'contrast(1.4) grayscale(1)';
    ctx.drawImage(video, boxX, boxY, boxWidth, boxHeight, 0, 0, boxWidth, boxHeight);
    ctx.filter = 'none';

    stopCamera();

    try {
      const Tesseract = await import('tesseract.js');
      const worker = await Tesseract.createWorker('jpn');
      const { data: { text } } = await worker.recognize(canvas);
      await worker.terminate();

      const cleanText = text.replace(/[\s\n\r]+/g, '').trim();
      setRecognizedText(cleanText);

      if (cleanText) {
        const tokens = tokenize(cleanText);
        const furigana = tokensToFurigana(tokens);
        setFuriganaData(furigana);
      }

      setShowResults(true);
    } catch (err) {
      console.error('OCR error:', err);
      alert('ã‚‚ã˜ã‚’ã€€ã‚ˆã¿ã¨ã‚Œã¾ã›ã‚“ã§ã—ãŸ');
    }

    setIsProcessing(false);
  };

  useEffect(() => {
    return () => {
      if (streamRef.current) {
        streamRef.current.getTracks().forEach((track) => track.stop());
      }
    };
  }, []);

  return (
    <div className="camera-mode">
      <div className="top-bar">
        <button className="back-btn" onClick={() => { stopCamera(); onBack(); }}>
          â† ã‚‚ã©ã‚‹
        </button>
        <h1>ã‚«ãƒ¡ãƒ©ãƒ¢ãƒ¼ãƒ‰ ğŸ“·</h1>
      </div>

      <div className="camera-area">
        <video
          ref={videoRef}
          autoPlay
          playsInline
          className={isStreaming ? 'visible' : 'hidden'}
        />
        {isStreaming && (
          <div className="scan-overlay">
            <div className="scan-box">
              <span className="scan-label">ã“ã“ã«ã€€ã‹ã‚“ã˜ã‚’ã€€ã†ã¤ã—ã¦ã­</span>
            </div>
          </div>
        )}
        <canvas ref={canvasRef} style={{ display: 'none' }} />
      </div>

      <div className="camera-controls">
        {!isStreaming && !isProcessing && (
          <button className="camera-start-btn" onClick={startCamera}>
            ğŸ“· ã‚«ãƒ¡ãƒ©ã‚’ã€€ã²ã‚‰ã
          </button>
        )}
        {isStreaming && (
          <button className="camera-stop-btn" onClick={stopAndAnalyze}>
            â¹ï¸ ã¨ã‚ã¦ã€€ã‚ˆã¿ã¨ã‚‹
          </button>
        )}
        {isProcessing && (
          <div className="processing">
            <div className="loading-spinner small" />
            <span>ã‚ˆã¿ã¨ã£ã¦ã„ã¾ã™...</span>
          </div>
        )}
      </div>

      {showResults && (
        <div className="results-area">
          <div className="results-header">
            <h2>ã‚ˆã¿ã¨ã£ãŸã€€ã‹ã‚“ã˜</h2>
            {recognizedText && (
              <button className="speak-btn" onClick={() => speak(recognizedText)}>
                ğŸ”Š
              </button>
            )}
          </div>

          {furiganaData.length > 0 ? (
            <div className="furigana-text large">
              {furiganaData.map((item, i) => (
                <span key={i} className="word">
                  {item.hasKanji ? (
                    <ruby>
                      {item.text}
                      <rt>{item.reading}</rt>
                    </ruby>
                  ) : (
                    item.text
                  )}
                </span>
              ))}
            </div>
          ) : (
            <p className="no-results">ã‹ã‚“ã˜ãŒã€€ã¿ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ</p>
          )}
        </div>
      )}
    </div>
  );
}
