import { useState, useRef, useEffect, useCallback } from 'react'
import { getFurigana, speak } from '../utils/japanese'

export default function CameraMode({ deeplKey, onBack }) {
  const [isStreaming, setIsStreaming] = useState(false);
  const [isAnalyzing, setIsAnalyzing] = useState(false);
  const [capturedImage, setCapturedImage] = useState(null);
  const [results, setResults] = useState([]);
  const videoRef = useRef(null);
  const canvasRef = useRef(null);
  const streamRef = useRef(null);

  const startCamera = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({
        video: { facingMode: 'environment', width: { ideal: 1280 }, height: { ideal: 720 } },
      });
      if (videoRef.current) {
        videoRef.current.srcObject = stream;
      }
      streamRef.current = stream;
      setIsStreaming(true);
      setCapturedImage(null);
      setResults([]);
    } catch (err) {
      console.error('Camera error:', err);
      alert('ã‚«ãƒ¡ãƒ©ã‚’ã€€ã²ã‚‰ã‘ã¾ã›ã‚“ã§ã—ãŸ');
    }
  };

  const stopCamera = useCallback(() => {
    if (streamRef.current) {
      streamRef.current.getTracks().forEach((track) => track.stop());
      streamRef.current = null;
    }
    setIsStreaming(false);
  }, []);

  const takePicture = useCallback(async () => {
    if (!videoRef.current || !canvasRef.current) return;

    const video = videoRef.current;
    const canvas = canvasRef.current;
    const ctx = canvas.getContext('2d');

    canvas.width = video.videoWidth;
    canvas.height = video.videoHeight;

    // Save a color version for display
    ctx.drawImage(video, 0, 0);
    const imageUrl = canvas.toDataURL('image/png');
    setCapturedImage(imageUrl);

    // Apply filters for OCR
    ctx.filter = 'contrast(1.5) grayscale(1)';
    ctx.drawImage(video, 0, 0);
    ctx.filter = 'none';

    stopCamera();
    setIsAnalyzing(true);

    try {
      // Send image to server-side OCR
      const imageData = canvas.toDataURL('image/png');
      const ocrRes = await fetch('/api/ocr', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ image: imageData }),
      });

      if (!ocrRes.ok) throw new Error(`OCR error: ${ocrRes.status}`);
      const { text: cleanText } = await ocrRes.json();

      if (cleanText && cleanText.length > 0) {
        const furigana = await getFurigana(cleanText);
        setResults([{ id: 1, text: cleanText, furigana }]);
      } else {
        setResults([]);
      }
    } catch (err) {
      console.error('OCR error:', err);
      setResults([]);
    }

    setIsAnalyzing(false);
  }, [stopCamera]);

  const handleBack = () => {
    stopCamera();
    onBack();
  };

  useEffect(() => {
    return () => {
      if (streamRef.current) {
        streamRef.current.getTracks().forEach((track) => track.stop());
      }
    };
  }, []);

  const allText = results.map((r) => r.text).join('');

  return (
    <div className="camera-mode">
      <div className="top-bar">
        <button className="back-btn" onClick={handleBack}>â† ã‚‚ã©ã‚‹</button>
        <h1>ã‚«ãƒ¡ãƒ©ãƒ¢ãƒ¼ãƒ‰ ğŸ“·</h1>
      </div>

      <div className="camera-area">
        <video
          ref={videoRef}
          autoPlay
          playsInline
          className={isStreaming ? 'visible' : 'hidden'}
        />
        {capturedImage && !isStreaming && (
          <div className="captured-preview">
            <img src={capturedImage} alt="ã¨ã£ãŸã€€ã—ã‚ƒã—ã‚“" />
          </div>
        )}
        <canvas ref={canvasRef} style={{ display: 'none' }} />
      </div>

      <div className="camera-controls">
        {!isStreaming && !capturedImage && !isAnalyzing && (
          <button className="camera-start-btn" onClick={startCamera}>
            ğŸ“· ã‚«ãƒ¡ãƒ©ã‚’ã€€ã²ã‚‰ã
          </button>
        )}
        {isStreaming && (
          <>
            <button className="camera-shutter-btn" onClick={takePicture}>
              ğŸ“¸
            </button>
            <span className="shutter-hint">ãƒœã‚¿ãƒ³ã‚’ã€€ãŠã—ã¦ã€€ã—ã‚ƒã—ã‚“ã‚’ã€€ã¨ã£ã¦ã­</span>
          </>
        )}
        {capturedImage && !isAnalyzing && (
          <button className="camera-start-btn" onClick={startCamera}>
            ğŸ“· ã‚‚ã†ã„ã¡ã©ã€€ã¨ã‚‹
          </button>
        )}
      </div>

      {isAnalyzing && (
        <div className="scan-status">
          <div className="loading-spinner small" />
          <span>ã‚ˆã¿ã¨ã£ã¦ã„ã¾ã™...</span>
        </div>
      )}

      {!isAnalyzing && results.length > 0 && (
        <div className="live-results">
          {results.map((result) => (
            <div key={result.id} className="live-result-item">
              <div className="live-result-content">
                <div className="furigana-text large">
                  {result.furigana.map((item, i) => (
                    <span key={i} className="word">
                      {item.hasKanji ? (
                        <ruby>
                          {item.text}
                          <rt>{item.reading}</rt>
                        </ruby>
                      ) : (
                        item.text
                      )}
                    </span>
                  ))}
                </div>
              </div>
              <button className="speak-btn" onClick={() => speak(result.text)}>
                ğŸ”Š
              </button>
            </div>
          ))}
        </div>
      )}

      {!isAnalyzing && capturedImage && results.length === 0 && (
        <div className="results-area">
          <p className="no-results">ã‹ã‚“ã˜ãŒã€€ã¿ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ</p>
        </div>
      )}

      {!isAnalyzing && allText && (
        <div className="camera-controls">
          <button className="speak-btn-large" onClick={() => speak(allText)}>
            ğŸ”Š ã‚ˆã‚€
          </button>
        </div>
      )}
    </div>
  );
}
